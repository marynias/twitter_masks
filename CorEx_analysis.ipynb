{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "import corextopic.corextopic as ct\n",
    "import corextopic.vis_topic as vt \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from corextopic import vis_topic as vt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in clean tweets, lemmatized and stop words removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clean_tweets_df = pd.read_csv(\"final_analysis_clean_tweets_lemmatized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1735605, 16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clean_tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the tweets into doc-word matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words present in more than 0.95 tweets. Word minimum frequency: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173509\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=.95,\n",
    "    min_df=3,\n",
    "    max_features=None,\n",
    "    ngram_range=(1, 2),\n",
    "    norm=None,\n",
    "    binary=True,\n",
    "    use_idf=False,\n",
    "    sublinear_tf=False\n",
    ")\n",
    "vectorizer = vectorizer.fit(my_clean_tweets_df['text'].values.astype('U'))\n",
    "tfidf = vectorizer.transform(my_clean_tweets_df['text'].values.astype('U'))\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define anchors to seed different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = [[\"carbon\", \"dioxide\", \"restrict\", \"deprivation\", \"deprive\", \n",
    "            \"flow\", \"cause\", \"suffocate\", \"toxic\", \"oxygen\", \"hypoxia\", \"exhale\", \"inhale\"], \n",
    "           [\"bacteria\", \"germ\", \"fungus\", \"fungal\", \"mold\", \"spore\", \"moisture\", \"humidity\", \"cause\", \"lung\",\n",
    "           \"breed\", \"contaminate\", \"moist\", \"humid\", \"pneumonia\", \"snot\", \"trap\"],\n",
    "           [\"mask\", \"ineffective\", \"useless\", \"inadequate\", \"flaw\", \"insufficient\",\n",
    "           \"pointless\", \"useless\", \"worthless\", \"futile\"],\n",
    "           [\"school\", \"communicate\", \"communication\", \"child abuse\", \"student\", \"pupil\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a model with a given number of topics and specified anchor strength "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_no = 200\n",
    "anchor_str = 10\n",
    "model = ct.Corex(n_hidden=topic_no, seed=42)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab,\n",
    "    anchors=anchors,\n",
    "    anchor_strength=anchor_str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Total Correlation (the bigger, the more informative the topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total correlation for individual topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Number of Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One way to choose the number of topics is to observe the distribution of TCs for each topic to see how much each additional topic contributes to the overall TC. We should keep adding topics until additional topics do not significantly contribute to the overall TC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(model.tcs.shape[0]), model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Which topics have TC above 0.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(model.tcs > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List top words for most popular topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get topics using the get_topics() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.get_topics()\n",
    "for topic_n,topic in enumerate(topics):\n",
    "    # w: word, mi: mutual information, s: sign\n",
    "    topic = [(w,mi,s) if s > 0 else ('~'+w,mi,s) for w,mi,s in topic]\n",
    "    # Unpack the info about the topic\n",
    "    words,mis,signs = zip(*topic)    \n",
    "    # Print topic\n",
    "    topic_str = str(topic_n+1)+': '+', '.join(words)\n",
    "    print(topic_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top tweets (indices in df) per given topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Documents are sorted according to log probabilities which is why the highest probability documents have a score of 0 ($e^0 = 1$) and other documents have negative scores (for example, $e^{-0.5} \\approx 0.6$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_top_docs(topic=0, n_docs=100, sort_by='log_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CorEx is a discriminative model, whereas LDA is a generative model. This means that while LDA outputs a probability distribution over each document, CorEx instead estimates the probability a document belongs to a topic given that document's words. As a result, the probabilities across topics for a given document do not have to add up to 1. The estimated probabilities of topics for each document can be accessed through log_p_y_given_x or p_y_given_x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a topic label for each tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also use a softmax to make a binary determination of which documents belong to each topic. These softmax labels can be accessed through labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly access the topic assignments for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most probable tweets for each topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs = model.get_top_docs()\n",
    "for topic_n, topic_docs in enumerate(top_docs):\n",
    "    docs,probs = zip(*topic_docs)\n",
    "    topic_str = str(topic_n+1)+': '+', '.join(str(docs))\n",
    "    print(topic_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get summary files and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt.vis_rep(model, column_label=vocab, prefix='topic-model-example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the output topic assignment with original tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = pd.DataFrame(\n",
    "    model.transform(tfidf), \n",
    "    columns=[\"topic_{}\".format(i+1) for i in range(topic_no)]\n",
    ").astype(float)\n",
    "topic_df.index = my_clean_tweets_df.index\n",
    "df = pd.concat([my_clean_tweets_df, topic_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic1_tweets = df[df.topic_1 == 1.0]\n",
    "topic1_tweets.to_csv(\"topic1_tweets_corex_partial.csv\")\n",
    "topic1_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic2_tweets = df[df.topic_2 == 1.0]\n",
    "topic2_tweets.to_csv(\"topic2_tweets_corex_partial.csv\")\n",
    "topic2_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic3_tweets = df[df.topic_3 == 1.0]\n",
    "topic3_tweets.to_csv(\"topic3_tweets_corex_partial.csv\")\n",
    "topic3_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic4_tweets = df[df.topic_4 == 1.0]\n",
    "topic4_tweets.to_csv(\"topic4_tweets_corex_partial.csv\")\n",
    "topic4_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic4_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
